#!/usr/bin/env python3
"""
æµ‹è¯• ck_tile_python æ¨¡å—çš„ Batched GEMM æµ‹è¯•è„šæœ¬
ä½¿ç”¨ M=512, K=2048, N=1024, batch_count=8
"""

import torch
import ck_tile_python
import time
import numpy as np

def test_module_import():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("=" * 60)
    print("æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    try:
        print(f"æˆåŠŸå¯¼å…¥ ck_tile_python æ¨¡å—")
        print(f"å¯ç”¨å‡½æ•°: {[f for f in dir(ck_tile_python) if not f.startswith('_')]}")
        return True
    except Exception as e:
        print(f"å¯¼å…¥å¤±è´¥: {e}")
        return False

def create_test_args():
    """åˆ›å»ºæµ‹è¯•å‚æ•°"""
    print("=" * 60)
    print("åˆ›å»ºæµ‹è¯•å‚æ•°...")
    try:
        args = ck_tile_python.BatchedGemmArgs()
        args.Ms = 512
        args.Ns = 1024
        args.Ks = 2048
        args.batched_count = 8
        args.dtype = "fp16"
        args.validate = True
        args.warmup = 2
        args.repeat = 5
        
        print(f"å‚æ•°åˆ›å»ºæˆåŠŸ:")
        print(f"  M (è¡Œæ•°): {args.Ms}")
        print(f"  N (åˆ—æ•°): {args.Ns}")
        print(f"  K (å†…ç§¯ç»´åº¦): {args.Ks}")
        print(f"  æ‰¹æ¬¡æ•°é‡: {args.batched_count}")
        print(f"  æ•°æ®ç±»å‹: {args.dtype}")
        print(f"  é¢„çƒ­æ¬¡æ•°: {args.warmup}")
        print(f"  é‡å¤æ¬¡æ•°: {args.repeat}")
        return args
    except Exception as e:
        print(f"å‚æ•°åˆ›å»ºå¤±è´¥: {e}")
        return None

def create_test_tensors(args):
    """åˆ›å»ºæµ‹è¯•å¼ é‡"""
    print("=" * 60)
    print("åˆ›å»ºæµ‹è¯•å¼ é‡...")
    try:
        M, N, K = args.Ms, args.Ns, args.Ks
        batch_count = args.batched_count
        dtype = torch.float16
        
        print(f"å¼ é‡è§„æ ¼:")
        print(f"  A: ({batch_count}, {M}, {K}) - æ‰¹æ¬¡çŸ©é˜µA")
        print(f"  B: ({batch_count}, {K}, {N}) - æ‰¹æ¬¡çŸ©é˜µB")
        print(f"  C: ({batch_count}, {M}, {N}) - æ‰¹æ¬¡çŸ©é˜µC")
        
        # åˆ›å»ºéšæœºæµ‹è¯•æ•°æ®
        A = torch.randn(batch_count, M, K, dtype=dtype, device='cuda')
        B = torch.randn(batch_count, K, N, dtype=dtype, device='cuda')
        
        print(f"\nå¼ é‡ä¿¡æ¯:")
        print(f"A tensor: shape={A.shape}, dtype={A.dtype}, device={A.device}")
        print(f"  stride: {A.stride()}")
        print(f"  is_contiguous: {A.is_contiguous()}")
        
        print(f"B tensor: shape={B.shape}, dtype={B.dtype}, device={B.device}")
        print(f"  stride: {B.stride()}")
        print(f"  is_contiguous: {B.is_contiguous()}")
        
        # è®¡ç®—å†…å­˜ä½¿ç”¨é‡
        A_memory = A.numel() * A.element_size() / 1024 / 1024  # MB
        B_memory = B.numel() * B.element_size() / 1024 / 1024  # MB
        print(f"\nå†…å­˜ä½¿ç”¨:")
        print(f"  A tensor: {A_memory:.2f} MB")
        print(f"  B tensor: {B_memory:.2f} MB")
        print(f"  æ€»è®¡: {A_memory + B_memory:.2f} MB")
        
        return A, B
    except Exception as e:
        print(f"å¼ é‡åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_batched_gemm_api(A_tensor, B_tensor, args):
    """æµ‹è¯• batched_gemm_api å‡½æ•°"""
    print("=" * 60)
    print("æµ‹è¯• batched_gemm_api å‡½æ•°...")
    try:
        print("è°ƒç”¨ ck_tile_python.batched_gemm_api...")
        
        # è°ƒç”¨æˆ‘ä»¬çš„å®ç°
        torch.cuda.synchronize()
        start_time = time.time()
        
        C_tensor = ck_tile_python.batched_gemm_api(A_tensor, B_tensor, args)
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        print(f"âœ… æ‰§è¡ŒæˆåŠŸï¼")
        print(f"  è€—æ—¶: {(end_time - start_time)*1000:.2f} ms")
        print(f"  è¿”å›å¼ é‡å½¢çŠ¶: {C_tensor.shape}")
        print(f"  è¿”å›å¼ é‡ç±»å‹: {C_tensor.dtype}")
        print(f"  è¿”å›å¼ é‡è®¾å¤‡: {C_tensor.device}")
        
        # æ£€æŸ¥ç»“æœå½¢çŠ¶
        expected_shape = (args.batched_count, args.Ms, args.Ns)
        actual_shape = C_tensor.shape
        print(f"\nå½¢çŠ¶éªŒè¯:")
        print(f"  æœŸæœ›å½¢çŠ¶: {expected_shape}")
        print(f"  å®é™…å½¢çŠ¶: {actual_shape}")
        
        if actual_shape == expected_shape:
            print(f"  âœ… å½¢çŠ¶æ­£ç¡®")
        else:
            print(f"  âŒ å½¢çŠ¶ä¸åŒ¹é…ï¼")
            return None
        
        return C_tensor
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def validate_results(A_tensor, B_tensor, C_tensor, args):
    """éªŒè¯ç»“æœæ­£ç¡®æ€§"""
    print("=" * 60)
    print("éªŒè¯ç»“æœæ­£ç¡®æ€§...")
    try:
        print("ä½¿ç”¨ PyTorch torch.bmm() è®¡ç®—å‚è€ƒç»“æœ...")
        
        # ä½¿ç”¨ PyTorch è®¡ç®—æœŸæœ›ç»“æœ
        torch.cuda.synchronize()
        start_time = time.time()
        expected = torch.bmm(A_tensor, B_tensor)  # æ‰¹æ¬¡çŸ©é˜µä¹˜æ³•
        torch.cuda.synchronize()
        pytorch_time = time.time() - start_time
        
        print(f"PyTorch è®¡ç®—è€—æ—¶: {pytorch_time*1000:.2f} ms")
        
        actual = C_tensor
        
        print(f"\nç»“æœæ¯”è¾ƒ:")
        print(f"  ck_tile ç»“æœå½¢çŠ¶: {actual.shape}")
        print(f"  PyTorch ç»“æœå½¢çŠ¶: {expected.shape}")
        print(f"  ck_tile æ•°æ®ç±»å‹: {actual.dtype}")
        print(f"  PyTorch æ•°æ®ç±»å‹: {expected.dtype}")
        
        # æ¯”è¾ƒç»“æœ
        print(f"\næ•°å€¼éªŒè¯:")
        is_close = torch.allclose(actual, expected, rtol=1e-3, atol=1e-3)
        
        if is_close:
            print("  âœ… ç»“æœæ­£ç¡®")
        else:
            print("  âŒ ç»“æœä¸æ­£ç¡®")
            
            # è®¡ç®—è¯¦ç»†å·®å¼‚
            diff = torch.abs(actual - expected)
            max_diff = torch.max(diff).item()
            mean_diff = torch.mean(diff).item()
            std_diff = torch.std(diff).item()
            
            print(f"  æœ€å¤§å·®å¼‚: {max_diff:.6f}")
            print(f"  å¹³å‡å·®å¼‚: {mean_diff:.6f}")
            print(f"  æ ‡å‡†å·®: {std_diff:.6f}")
            
            # æ£€æŸ¥æœ‰å¤šå°‘å…ƒç´ å·®å¼‚è¾ƒå¤§
            large_diff_count = torch.sum(diff > 1e-2).item()
            total_elements = diff.numel()
            large_diff_ratio = large_diff_count / total_elements * 100
            
            print(f"  å·®å¼‚ > 1e-2 çš„å…ƒç´ : {large_diff_count}/{total_elements} ({large_diff_ratio:.2f}%)")
            
            return False
        
        return True
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def benchmark_performance(A_tensor, B_tensor, args):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("=" * 60)
    print("æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    try:
        # è®¡ç®—ç†è®ºæ€§èƒ½æŒ‡æ ‡
        total_flops = 2.0 * args.batched_count * args.Ms * args.Ns * args.Ks
        print(f"ç†è®ºè®¡ç®—é‡: {total_flops/1e12:.2f} TFLOPs")
        
        # æµ‹è¯• ck_tile_python å®ç°
        print(f"\næµ‹è¯• ck_tile_python å®ç° ({args.repeat} æ¬¡)...")
        torch.cuda.synchronize()
        start_time = time.time()
        
        for i in range(args.repeat):
            C_tensor = ck_tile_python.batched_gemm_api(A_tensor, B_tensor, args)
            if i == 0:  # ç¬¬ä¸€æ¬¡çš„ç»“æœç”¨äºéªŒè¯
                first_result = C_tensor.clone()
        
        torch.cuda.synchronize()
        ck_tile_time = time.time() - start_time
        
        # æµ‹è¯• PyTorch å®ç°
        print(f"æµ‹è¯• PyTorch å®ç° ({args.repeat} æ¬¡)...")
        torch.cuda.synchronize()
        start_time = time.time()
        
        for i in range(args.repeat):
            pytorch_result = torch.bmm(A_tensor, B_tensor)
        
        torch.cuda.synchronize()
        pytorch_time = time.time() - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        ck_tile_avg_time = ck_tile_time / args.repeat
        pytorch_avg_time = pytorch_time / args.repeat
        
        ck_tile_gflops = (total_flops / ck_tile_avg_time) / 1e9
        pytorch_gflops = (total_flops / pytorch_avg_time) / 1e9
        
        print(f"\næ€§èƒ½å¯¹æ¯”ç»“æœ:")
        print(f"  ck_tile_python:")
        print(f"    å¹³å‡è€—æ—¶: {ck_tile_avg_time*1000:.2f} ms/iter")
        print(f"    æ€§èƒ½: {ck_tile_gflops:.2f} GFLOPS")
        print(f"    ååé‡: {total_flops/ck_tile_avg_time/1e12:.2f} TFLOPs/s")
        
        print(f"  PyTorch:")
        print(f"    å¹³å‡è€—æ—¶: {pytorch_avg_time*1000:.2f} ms/iter")
        print(f"    æ€§èƒ½: {pytorch_gflops:.2f} GFLOPS")
        print(f"    ååé‡: {total_flops/pytorch_avg_time/1e12:.2f} TFLOPs/s")
        
        speedup = pytorch_avg_time / ck_tile_avg_time
        print(f"\nåŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        if speedup > 1.0:
            print(f"  âœ… ck_tile_python æ¯” PyTorch å¿« {speedup:.2f}x")
        else:
            print(f"  âš ï¸  PyTorch æ¯” ck_tile_python å¿« {1/speedup:.2f}x")
        
        return first_result
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("=" * 60)
    print("å†…å­˜ä½¿ç”¨æµ‹è¯•...")
    try:
        # è·å–åˆå§‹å†…å­˜çŠ¶æ€
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # åˆ›å»ºå¤§å¼ é‡
        batch_count, M, N, K = 8, 512, 1024, 2048
        A = torch.randn(batch_count, M, K, dtype=torch.float16, device='cuda')
        B = torch.randn(batch_count, K, N, dtype=torch.float16, device='cuda')
        
        after_creation = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # æ‰§è¡Œè®¡ç®—
        args = ck_tile_python.BatchedGemmArgs()
        args.Ms, args.Ns, args.Ks = M, N, K
        args.batched_count = batch_count
        args.dtype = "fp16"
        args.warmup = 1
        args.repeat = 1
        
        C = ck_tile_python.batched_gemm_api(A, B, args)
        
        after_computation = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        print(f"å†…å­˜ä½¿ç”¨æƒ…å†µ:")
        print(f"  åˆå§‹å†…å­˜: {initial_memory:.2f} MB")
        print(f"  åˆ›å»ºå¼ é‡å: {after_creation:.2f} MB")
        print(f"  è®¡ç®—å®Œæˆå: {after_computation:.2f} MB")
        print(f"  å¼ é‡å ç”¨: {after_creation - initial_memory:.2f} MB")
        print(f"  è®¡ç®—é¢å¤–å ç”¨: {after_computation - after_creation:.2f} MB")
        
        # æ¸…ç†
        del A, B, C
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯• ck_tile_python Batched GEMM æ¨¡å—")
    print("ğŸ“Š æµ‹è¯•è§„æ ¼: M=512, K=2048, N=1024, batch_count=8")
    print("=" * 60)
    
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
        return
    
    print(f"âœ… CUDA å¯ç”¨ï¼Œè®¾å¤‡: {torch.cuda.get_device_name()}")
    print(f"   æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"   å½“å‰æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
    
    # æµ‹è¯•æ¨¡å—å¯¼å…¥
    if not test_module_import():
        return
    
    # åˆ›å»ºæµ‹è¯•å‚æ•°
    args = create_test_args()
    if args is None:
        return
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    A_tensor, B_tensor = create_test_tensors(args)
    if A_tensor is None or B_tensor is None:
        return
    
    # æµ‹è¯•ä¸»è¦åŠŸèƒ½
    C_tensor = test_batched_gemm_api(A_tensor, B_tensor, args)
    if C_tensor is None:
        return
    
    # éªŒè¯ç»“æœ
    if args.validate:
        is_correct = validate_results(A_tensor, B_tensor, C_tensor, args)
        if not is_correct:
            print("âŒ éªŒè¯å¤±è´¥ï¼Œç»“æœä¸æ­£ç¡®")
            return
    
    # æ€§èƒ½æµ‹è¯•
    benchmark_result = benchmark_performance(A_tensor, B_tensor, args)
    if benchmark_result is None:
        return
    
    # å†…å­˜ä½¿ç”¨æµ‹è¯•
    test_memory_usage()
    
    print("=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“ˆ æµ‹è¯•æ€»ç»“:")
    print("   âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
    print("   âœ… å‚æ•°åˆ›å»ºæˆåŠŸ") 
    print("   âœ… å¼ é‡åˆ›å»ºæˆåŠŸ")
    print("   âœ… API è°ƒç”¨æˆåŠŸ")
    if args.validate:
        print("   âœ… ç»“æœéªŒè¯é€šè¿‡")
    print("   âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
    print("   âœ… å†…å­˜æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()
