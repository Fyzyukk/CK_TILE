// !!! This is a file automatically generated by hipify!!!
#include "group_gemm_hip.hpp"
// #include "../../kernels/group_gemm_kernel/grouped_gemm_kernel_hip.cpp" 

extern "C" float grouped_gemm_fp16_fp16_fp16(
    const std::vector<grouped_gemm_kargs>& gemm_descs,
    const ck_tile::stream_config& s,
    void* kargs_ptr);

template <typename Layout>
static constexpr inline auto is_row_major(Layout layout_)
{
    return ck_tile::bool_constant<std::is_same_v<ck_tile::remove_cvref_t<decltype(layout_)>,
                                                 ck_tile::tensor_layout::gemm::RowMajor>>{};
}

namespace ck_tile_cpp {

namespace group_gemm {

std::vector<torch::Tensor> grouped_gemm_api(
    std::vector<torch::Tensor>& A_tensors,
    std::vector<torch::Tensor>& B_tensors,
    const GroupGemmArgs& args) {
    
    TORCH_CHECK(args.Ms.size() == args.Ns.size() && args.Ns.size() == args.Ks.size(),
                "Ms, Ns, Ks must have the same length");
    
    TORCH_CHECK(A_tensors.size() == args.Ms.size(), 
                "Number of A tensors must match number of dimensions");
    TORCH_CHECK(B_tensors.size() == args.Ms.size(), 
                "Number of B tensors must match number of dimensions");
    
    TORCH_CHECK(args.group_count == args.Ms.size(),
                "group_count must match number of dimensions");
    
    for (size_t i = 0; i < A_tensors.size(); ++i) {
        TORCH_CHECK(A_tensors[i].is_cuda(), "All A tensors must be on CUDA device");
        if (!A_tensors[i].is_contiguous()) {
            A_tensors[i] = A_tensors[i].contiguous();
        }
    }
    
    for (int i = 0; i < args.group_count; ++i) {
        int M = args.Ms[i];
        int K = args.Ks[i];
        int N = args.Ns[i];
        
        TORCH_CHECK(A_tensors[i].size(0) == M && A_tensors[i].size(1) == K,
                    "A tensor ", i, " shape mismatch: expected (" , M, ", ", K
                    , "), got (" , A_tensors[i].size(0) , ", " , A_tensors[i].size(1) , ")");
        
        TORCH_CHECK(B_tensors[i].size(0) == K && B_tensors[i].size(1) == N,
                    "B tensor " , i , " shape mismatch: expected (" , K, ", " , N 
                    , "), got (" , B_tensors[i].size(0) , ", " , B_tensors[i].size(1) , ")");
    }

    for (size_t i = 0; i < B_tensors.size(); ++i) {
        TORCH_CHECK(B_tensors[i].is_cuda(), "All B tensors must be on CUDA device");
        if (B_tensors[i].stride(0) == 1 && B_tensors[i].stride(1) == B_tensors[i].size(0)) {
        } else {
            B_tensors[i] = B_tensors[i].t().contiguous();
        }
    }
    
    std::vector<torch::Tensor> C_tensors;
    C_tensors.reserve(args.group_count);
    
    for (int i = 0; i < args.group_count; ++i) {
        int M = args.Ms[i];
        int N = args.Ns[i];
        
        auto C = torch::zeros({M, N}, 
                              torch::TensorOptions()
                              .dtype(A_tensors[i].dtype())
                              .device(A_tensors[i].device())
                              .memory_format(torch::MemoryFormat::Contiguous));
        C_tensors.push_back(C);
    }
    
    std::vector<grouped_gemm_kargs> gemm_descs;
    gemm_descs.reserve(args.group_count);
    
    for (int i = 0; i < args.group_count; ++i) {
        const int M = args.Ms[i];
        const int N = args.Ns[i];
        const int K = args.Ks[i];
        
        int stride_A = ck_tile::get_default_stride(M, K, 0, ck_tile::bool_constant<true>{});  // Row major
        int stride_B = ck_tile::get_default_stride(K, N, 0, ck_tile::bool_constant<false>{}); // Column major
        int stride_C = ck_tile::get_default_stride(M, N, 0, ck_tile::bool_constant<true>{});  // Row major
        
        grouped_gemm_kargs desc;
        desc.a_ptr = A_tensors[i].data_ptr();
        desc.b_ptr = B_tensors[i].data_ptr();
        desc.e_ptr = C_tensors[i].data_ptr();  
        desc.M = M;
        desc.N = N;
        desc.K = K;
        desc.stride_A = stride_A;
        desc.stride_B = stride_B;
        desc.stride_E = stride_C; 
        desc.k_batch = 1;  
        gemm_descs.push_back(desc);
    }
    
    ck_tile::stream_config s{nullptr, true, 1, args.warmup, args.repeat};
    
    size_t workspace_size = gemm_descs.size() * sizeof(ck_tile::GemmTransKernelArg);
    void* kargs_ptr;
    hipError_t hip_error = hipMalloc(&kargs_ptr, workspace_size);
    if (hip_error != hipSuccess) {
        throw std::runtime_error("Failed to allocate workspace memory");
    }
    
    try {
        if (A_tensors[0].dtype() == torch::kFloat16) {
            grouped_gemm_fp16_fp16_fp16(gemm_descs, s, kargs_ptr);
        } else {
            for (size_t i = 0; i < args.Ms.size(); ++i) {
                C_tensors[i] = torch::mm(A_tensors[i], B_tensors[i]);
            }
        }
    } catch (const std::exception& e) {
        std::cerr << "Kernel execution failed: " << e.what() << std::endl;
        for (size_t i = 0; i < args.Ms.size(); ++i) {
            C_tensors[i] = torch::mm(A_tensors[i], B_tensors[i]);
        }
    }
    
    (void)hipFree(kargs_ptr);
    
    return C_tensors;
}

void register_group_gemm_apis(pybind11::module& m) {
    pybind11::class_<GroupGemmArgs>(m, "GroupGemmArgs")
        .def(pybind11::init<>())
        .def_readwrite("Ms", &GroupGemmArgs::Ms)
        .def_readwrite("Ns", &GroupGemmArgs::Ns)
        .def_readwrite("Ks", &GroupGemmArgs::Ks)
        .def_readwrite("group_count", &GroupGemmArgs::group_count)
        .def_readwrite("dtype", &GroupGemmArgs::dtype)
        .def_readwrite("validate", &GroupGemmArgs::validate)
        .def_readwrite("warmup", &GroupGemmArgs::warmup)
        .def_readwrite("repeat", &GroupGemmArgs::repeat);
    
    m.def("grouped_gemm_api", &grouped_gemm_api, 
          "Perform grouped GEMM operations with interface",
          pybind11::arg("A_tensors"), pybind11::arg("B_tensors"), pybind11::arg("args"));
    
}

} // namespace group_gemm

} // namespace ck_tile_cpp
