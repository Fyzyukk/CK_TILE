// !!! This is a file automatically generated by hipify!!!
#include "gemm_hip.hpp"
// #include "../../kernels/gemm_kernel/gemm_kernel_hip.cpp" 
#include <hip/hip_runtime.h>
#include <iostream>

extern "C" float gemm_fp16_fp16_fp16(
    const ck_tile::GemmHostArgs<>& args,
    const ck_tile::stream_config& s,
    const bool persistent);

extern "C" float gemm_bf16_bf16_bf16(
    const ck_tile::GemmHostArgs<>& args,
    const ck_tile::stream_config& s,
    const bool persistent);

extern "C" float gemm_fp8_fp8_fp16(
    const ck_tile::GemmHostArgs<>& args,
    const ck_tile::stream_config& s,
    const bool persistent);

extern "C" float gemm_bf8_bf8_fp16(
    const ck_tile::GemmHostArgs<>& args,
    const ck_tile::stream_config& s,
    const bool persistent);

extern "C" float gemm_fp16_pk_int4_fp16(
    const ck_tile::GemmHostArgs<>& args,
    const ck_tile::stream_config& s,
    const bool persistent);

extern "C" float gemm_int8_int8_int32(
    const ck_tile::GemmHostArgs<>& args,
    const ck_tile::stream_config& s,
    const bool persistent);


namespace ck_tile_cpp {

namespace gemm {

torch::Tensor gemm_api(
    torch::Tensor& A_tensor,
    torch::Tensor& B_tensor,
    const GemmArgs& args) {

    TORCH_CHECK(A_tensor.is_cuda(), "A tensors must be on device");
    if (!A_tensor.is_contiguous()) {
            A_tensor = A_tensor.contiguous();
    }

    TORCH_CHECK(B_tensor.is_cuda(), "B tensors must be on device");
    if (B_tensor.stride(0) == 1 && B_tensor.stride(1) == B_tensor.size(0)) {
        // Column Major
    } else {
        // Column Major
        B_tensor = B_tensor.t().contiguous();
    }

    const int M = args.Ms;
    const int N = args.Ns;
    const int K = args.Ks;
    torch::ScalarType c_dtype;
    if (args.dtype == "fp16") {
        c_dtype = torch::kFloat16;
    } 
    else if (args.dtype == "bf16") 
    {
        c_dtype = static_cast<torch::ScalarType>(c10::ScalarType::BFloat16);
    }
    else if (args.dtype == "int32")
    {
        c_dtype = torch::kInt32; 
    }
    auto C_tensor = torch::zeros({M, N}, 
                                torch::TensorOptions()
                                 .dtype(c_dtype)
                                 .device(A_tensor.device())
                                 .memory_format(torch::MemoryFormat::Contiguous));
    
    ck_tile::GemmHostArgs<> gemm_desc;

    int stride_A = ck_tile::get_default_stride(M, K, 0, ck_tile::bool_constant<true>{});
    int stride_B = ck_tile::get_default_stride(K, N, 0, ck_tile::bool_constant<false>{}); 
    int stride_C = ck_tile::get_default_stride(M, N, 0, ck_tile::bool_constant<true>{});

    gemm_desc.a_ptr = A_tensor.data_ptr();
    gemm_desc.b_ptr = B_tensor.data_ptr();
    gemm_desc.e_ptr = C_tensor.data_ptr();
    gemm_desc.M = M;
    gemm_desc.N = N;
    gemm_desc.K = K;
    gemm_desc.stride_A = stride_A;
    gemm_desc.stride_B = stride_B;
    gemm_desc.stride_E = stride_C;
    gemm_desc.k_batch = 1;

    ck_tile::stream_config s{nullptr, true, 1, args.warmup, args.repeat};
    bool persistent = args.persistent;
    
    const auto kBF16 = static_cast<torch::ScalarType>(c10::ScalarType::BFloat16);
    const auto kFP8 = static_cast<torch::ScalarType>(c10::ScalarType::Float8_e4m3fn);
    const auto kBF8 = static_cast<torch::ScalarType>(c10::ScalarType::Float8_e5m2);

    if (A_tensor.dtype() == torch::kFloat16 && B_tensor.dtype() == torch::kFloat16 && C_tensor.dtype() == torch::kFloat16) {
        gemm_fp16_fp16_fp16(gemm_desc, s, persistent);
    } 
    else if (A_tensor.dtype() == kBF16 && B_tensor.dtype() == kBF16 && C_tensor.dtype() == kBF16) {
        gemm_bf16_bf16_bf16(gemm_desc, s, persistent);
    }
    else if (A_tensor.dtype() == kFP8 && B_tensor.dtype() == kFP8 && C_tensor.dtype() == torch::kFloat16) {
        gemm_fp8_fp8_fp16(gemm_desc, s, persistent);
    }
    else if (A_tensor.dtype() == kBF8 && B_tensor.dtype() == kBF8 && C_tensor.dtype() == torch::kFloat16) {
        gemm_bf8_bf8_fp16(gemm_desc, s, persistent);
    }
    else if (A_tensor.dtype() == torch::kInt8 && B_tensor.dtype() == torch::kInt8 && C_tensor.dtype() == torch::kInt32) {
        gemm_int8_int8_int32(gemm_desc, s, persistent);
    }
    else{
        TORCH_CHECK(false, "Unsupported dtype combination for GEMM");
    }

    return C_tensor;
    
}

void register_gemm_apis(pybind11::module& m) {
    pybind11::class_<GemmArgs>(m, "GemmArgs")
        .def(pybind11::init<>())
        .def_readwrite("Ms", &GemmArgs::Ms)
        .def_readwrite("Ns", &GemmArgs::Ns)
        .def_readwrite("Ks", &GemmArgs::Ks)
        .def_readwrite("dtype", &GemmArgs::dtype)
        .def_readwrite("validate", &GemmArgs::validate)
        .def_readwrite("warmup", &GemmArgs::warmup)
        .def_readwrite("repeat", &GemmArgs::repeat)
        .def_readwrite("persistent", &GemmArgs::persistent);
    

    m.def("gemm_api", &gemm_api, 
          "Perform GEMM operations with interface",
          pybind11::arg("A_tensor"), pybind11::arg("B_tensor"), pybind11::arg("args"));
    
}


} // namespace gemm

} // namespace ck_tile_cpp