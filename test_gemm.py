#!/usr/bin/env python3
"""
æµ‹è¯• ck_tile_python æ¨¡å—çš„ GEMM æµ‹è¯•è„šæœ¬
"""

import torch
import ck_tile_python
import time

def test_module_import():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("=" * 50)
    print("æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    try:
        print(f"æˆåŠŸå¯¼å…¥ ck_tile_python æ¨¡å—")
        print(f"å¯ç”¨å‡½æ•°: {dir(ck_tile_python)}")
        return True
    except Exception as e:
        print(f"å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_gemm_args():
    """æµ‹è¯• GemmArgs ç»“æ„"""
    print("=" * 50)
    print("æµ‹è¯• GemmArgs ç»“æ„...")
    try:
        # åˆ›å»ºå‚æ•°
        args = ck_tile_python.GemmArgs()
        args.Ms = 2048
        args.Ns = 2048
        args.Ks = 2048
        args.dtype = "fp16"
        args.validate = True
        args.warmup = 2
        args.repeat = 5
        args.persistent = False
        
        print(f"å‚æ•°åˆ›å»ºæˆåŠŸ:")
        print(f"  Ms: {args.Ms}")
        print(f"  Ns: {args.Ns}")
        print(f"  Ks: {args.Ks}")
        print(f"  dtype: {args.dtype}")
        print(f"  persistent: {args.persistent}")
        return args
    except Exception as e:
        print(f"å‚æ•°åˆ›å»ºå¤±è´¥: {e}")
        return None

def create_test_tensors(args):
    """åˆ›å»ºæµ‹è¯•å¼ é‡"""
    print("=" * 50)
    print("åˆ›å»ºæµ‹è¯•å¼ é‡...")
    try:
        # ç¡®å®šæ•°æ®ç±»å‹
        dtype = torch.float16 if args.dtype == "fp16" else torch.float32
        
        M, N, K = args.Ms, args.Ns, args.Ks
        
        # åˆ›å»ºéšæœºæµ‹è¯•æ•°æ®
        A = torch.randn(M, K, dtype=dtype, device='cuda')
        B = torch.randn(K, N, dtype=dtype, device='cuda')
        
        print(f"çŸ©é˜µå°ºå¯¸: A({M}x{K}), B({K}x{N}) -> C({M}x{N})")
        print(f"A tensor: shape={A.shape}, stride={A.stride()}")
        print(f"B tensor: shape={B.shape}, stride={B.stride()}")
        
        return A, B
    except Exception as e:
        print(f"å¼ é‡åˆ›å»ºå¤±è´¥: {e}")
        return None, None

def test_gemm_api(A_tensor, B_tensor, args):
    """æµ‹è¯• gemm_api å‡½æ•°"""
    print("=" * 50)
    print("æµ‹è¯• gemm_api å‡½æ•°...")
    try:
        # è°ƒç”¨æˆ‘ä»¬çš„å®ç°
        start_time = time.time()
        C_tensor = ck_tile_python.gemm_api(A_tensor, B_tensor, args)
        end_time = time.time()
        
        print(f"æ‰§è¡ŒæˆåŠŸï¼è€—æ—¶: {end_time - start_time:.4f} ç§’")
        print(f"è¿”å›ç»“æœå¼ é‡å½¢çŠ¶: {C_tensor.shape}")
        
        # æ£€æŸ¥ç»“æœå½¢çŠ¶
        expected_M, expected_N = args.Ms, args.Ns
        actual_M, actual_N = C_tensor.shape
        print(f"æœŸæœ›å½¢çŠ¶ ({expected_M}, {expected_N}), å®é™…å½¢çŠ¶ ({actual_M}, {actual_N})")
        
        if (actual_M, actual_N) != (expected_M, expected_N):
            print(f"âŒ å½¢çŠ¶ä¸åŒ¹é…ï¼")
            return None
        else:
            print(f"âœ… å½¢çŠ¶æ­£ç¡®")
        
        return C_tensor
    except Exception as e:
        print(f"æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def validate_results(A_tensor, B_tensor, C_tensor, args):
    """éªŒè¯ç»“æœæ­£ç¡®æ€§"""
    print("=" * 50)
    print("éªŒè¯ç»“æœæ­£ç¡®æ€§...")
    try:
        # ä½¿ç”¨ PyTorch è®¡ç®—æœŸæœ›ç»“æœ
        expected = torch.mm(A_tensor, B_tensor)
        actual = C_tensor
        
        # æ¯”è¾ƒç»“æœ
        is_close = torch.allclose(actual, expected, rtol=1e-3, atol=1e-3)
        
        if is_close:
            print("âœ… ç»“æœæ­£ç¡®")
        else:
            print("âŒ ç»“æœä¸æ­£ç¡®")
            # è®¡ç®—å·®å¼‚
            diff = torch.abs(actual - expected)
            max_diff = torch.max(diff).item()
            mean_diff = torch.mean(diff).item()
            print(f"  æœ€å¤§å·®å¼‚: {max_diff:.6f}")
            print(f"  å¹³å‡å·®å¼‚: {mean_diff:.6f}")
            return False
        
        return True
    except Exception as e:
        print(f"éªŒè¯å¤±è´¥: {e}")
        return False

def benchmark_comparison(A_tensor, B_tensor, args):
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("=" * 50)
    print("æ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
    try:
        # æµ‹è¯•æˆ‘ä»¬çš„å®ç°
        print("æµ‹è¯• ck_tile_python å®ç°...")
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(args.repeat):
            C_tensor = ck_tile_python.gemm_api(A_tensor, B_tensor, args)
        
        torch.cuda.synchronize()
        ck_tile_time = time.time() - start_time
        
        # æµ‹è¯• PyTorch å®ç°
        print("æµ‹è¯• PyTorch å®ç°...")
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(args.repeat):
            pytorch_result = torch.mm(A_tensor, B_tensor)
        
        torch.cuda.synchronize()
        pytorch_time = time.time() - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_flops = 2.0 * args.Ms * args.Ns * args.Ks
        
        ck_tile_gflops = (total_flops / ck_tile_time) / 1e9
        pytorch_gflops = (total_flops / pytorch_time) / 1e9
        
        print(f"ç»“æœå¯¹æ¯”:")
        print(f"  ck_tile_python: {ck_tile_time/args.repeat*1000:.2f} ms/iter, {ck_tile_gflops:.2f} GFLOPS")
        print(f"  PyTorch:        {pytorch_time/args.repeat*1000:.2f} ms/iter, {pytorch_gflops:.2f} GFLOPS")
        print(f"  åŠ é€Ÿæ¯”:         {pytorch_time/ck_tile_time:.2f}x")
        
        return True
    except Exception as e:
        print(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_fp16_only():
    """æµ‹è¯• fp16 æ•°æ®ç±»å‹"""
    print("=" * 50)
    print("æµ‹è¯• fp16 æ•°æ®ç±»å‹...")
    
    try:
        # åˆ›å»ºå‚æ•°
        args = ck_tile_python.GemmArgs()
        args.Ms = 512
        args.Ns = 512
        args.Ks = 512
        args.dtype = "fp16"
        args.validate = True
        args.warmup = 1
        args.repeat = 1
        args.persistent = False
        
        # åˆ›å»º fp16 å¼ é‡
        A = torch.randn(args.Ms, args.Ks, dtype=torch.float16, device='cuda')
        B = torch.randn(args.Ks, args.Ns, dtype=torch.float16, device='cuda')
        
        print(f"åˆ›å»º fp16 å¼ é‡: A({args.Ms}x{args.Ks}), B({args.Ks}x{args.Ns})")
        print(f"A tensor: shape={A.shape}, dtype={A.dtype}")
        print(f"B tensor: shape={B.shape}, dtype={B.dtype}")
        
        # æµ‹è¯•
        C = ck_tile_python.gemm_api(A, B, args)
        print(f"âœ… fp16 GEMM æµ‹è¯•æˆåŠŸ")
        print(f"ç»“æœå¼ é‡: shape={C.shape}, dtype={C.dtype}")
        
        return True
        
    except Exception as e:
        print(f"âŒ fp16 GEMM æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯• ck_tile_python GEMM æ¨¡å—")
    print("=" * 50)
    
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
        return
    
    print(f"âœ… CUDA å¯ç”¨ï¼Œè®¾å¤‡: {torch.cuda.get_device_name()}")
    
    # æµ‹è¯•æ¨¡å—å¯¼å…¥
    if not test_module_import():
        return
    
    # æµ‹è¯•å‚æ•°åˆ›å»º
    args = test_gemm_args()
    if args is None:
        return
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    A_tensor, B_tensor = create_test_tensors(args)
    if A_tensor is None or B_tensor is None:
        return
    
    # æµ‹è¯•ä¸»è¦åŠŸèƒ½
    C_tensor = test_gemm_api(A_tensor, B_tensor, args)
    if C_tensor is None:
        return
    
    # éªŒè¯ç»“æœ
    if args.validate:
        is_correct = validate_results(A_tensor, B_tensor, C_tensor, args)
        if not is_correct:
            print("âŒ éªŒè¯å¤±è´¥ï¼Œç»“æœä¸æ­£ç¡®")
            return
    
    # æ€§èƒ½æµ‹è¯•
    benchmark_comparison(A_tensor, B_tensor, args)
    
    # æµ‹è¯• fp16 æ•°æ®ç±»å‹
    test_fp16_only()
    
    print("=" * 50)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()
